<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>VA2CONTACT: Visual-auditory Extrinsic Contact Estimation</title>

    <meta name="description" content="VA2CONTACT: Visual-auditory Extrinsic Contact Estimation">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://va2contact.github.io/img/teaser.jpg">
    <meta property="og:image:type" content="image/jpg">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://va2contact.github.io/" />
    <meta property="og:title" content="VA2CONTACT: Visual-auditory Extrinsic Contact Estimation" />
    <meta property="og:description"
        content="VA2CONTACT is a novel visual-auditory method for extrinsic contact estimation that integrates global scene information from vision with local contact cues obtained through active audio sensing." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="VA2CONTACT: Visual-auditory Extrinsic Contact Estimation" />
    <meta name="twitter:description"
        content="VA2CONTACT is a novel visual-auditory method for extrinsic contact estimation that integrates global scene information from vision with local contact cues obtained through active audio sensing." />
    <meta name="twitter:image" content="https://va2contact.github.io/img/teaser.jpg" />

    <link rel="icon" href="favicon.ico">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
    <script defer src="js/app.js"></script>
</head>

<body style="padding: 5%; padding-top: min(15px, 5%); padding-bottom: min(5px, 5%); width: 100%">
    <div class="container-lg text-center" style="max-width: 1500px; margin: auto;" id="main">
        <header role="banner">
            <div class="row">
                <h2 class="col-md-12 text-center">
                    <b>VA2CONTACT:</b> Visual-auditory Extrinsic Contact Estimation</br>
                </h2>
            </div>
            <div class="row text-center">
                <div class="col-md-3">
                </div>
                <div class="container-fluid text-center">
                    <ul class="list-inline" style="white-space: nowrap; margin:0px 0px 0px 0px;">
                        <li><a style="font-size: calc(min(3vw, 15px))" href="mailto:yixili@umich.edu">Xili Yi¹</a></li>
                        <li><a style="font-size: calc(min(3vw, 15px))" href="mailto:jayjun@umich.edu">Jayjun Lee¹</a></li>
                        <li><a style="font-size: calc(min(3vw, 15px))" href="mailto:nfz@umich.edu">Nima Fazeli¹</a></li>
                    </ul>
                </div>
                <div class="container-fluid text-center">
                    <ul class="list-inline" style="white-space: nowrap; margin:0px 0px 0px 0px;">
                        <li><a style="font-size: calc(min(3vw, 15px))">¹University of Michigan</a></li>
                    </ul>
                </div>
            </div>
            <div class="row text-center">
                <span>
                    <a href="#" class="button"
                        style="width: 80px; font-size: 15px">
                        <span class="icon">
                            <i class="fa fa-file-pdf-o"></i>
                        </span>
                        <span>Paper</span>
                    </a>
                </span>
                <span>
                    <a href="#" class="button"
                        style="width: 80px; font-size: 15px">
                        <span class="icon">
                            <i class="fa fa-github"></i>
                        </span>
                        <span>Code</span>
                    </a>
                </span>
                <span>
                    <a href="#" class="button"
                        style="width: 80px; font-size: 15px">
                        <span class="icon">
                            <i class="fa fa-database"></i>
                        </span>
                        <span>Data</span>
                    </a>
                </span>
            </div>
        </header>

        <main role="main">
            <!-- Teaser Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <img src="img/teaser.jpg" alt="VA2CONTACT Teaser" class="img-fluid" style="width: 60%; margin: 0 auto 20px auto; display: block;">
                </div>
            </div>

            <!-- Video Demo Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Video Demonstration</h3>
                    <div class="row">
                        <div class="col-md-8 offset-md-2">
                            <div class="ratio ratio-16x9" style="margin-bottom: 20px;">
                                <video controls width="100%" poster="img/video_thumbnail.jpg">
                                    <source src="videos/demo1.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <p class="text-center">Contact Estimation Demo</p>
                        </div>
                    </div>
                </div>
            </div><br>

            <!-- TL;DR Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2 rounded highlight-section">
                    <h6 style="text-align: center; color:rgb(0, 0, 0)"><strong>TL;DR</strong>: VA2CONTACT integrates visual and active audio sensing to accurately detect extrinsic contacts between a grasped object and the environment, even in occluded or ambiguous scenarios, with zero-shot sim-to-real transfer.</h6>
                </div>
            </div><br><br>

            <!-- Abstract Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Abstract</h3>
                    <p style="text-align: justify;">
                        Extrinsic contact estimation is a crucial capability for robots to accurately understand how tools interact with their environment. Vision-based methods allow observation of the entire scene, but they often fall short in providing sufficient local information, particularly when contacts are occluded or within the resolution limits of the sensor. While tactile sensors or force/torque sensors offer precise measurements of direct contact surfaces, they struggle to perceive indirect contacts, such as those between a tool and the environment.
                    </p>
                    <p style="text-align: justify;">
                        To address this challenge, we propose a novel visual-auditory extrinsic contact estimation method, VA2CONTACT. Our approach integrates global visual feedback with local information obtained through active audio sensing. This enables the estimation of extrinsic contacts as masks in 2D image space, from which both the contact location and type can be inferred. We equip a robotic gripper with contact microphones and conduction speakers, enabling the system to emit and receive acoustic signals through the grasped object to detect external contacts.
                    </p>
                    <p style="text-align: justify;">
                        We train our perception pipeline entirely in simulation and achieve zero-shot transfer to the real-world. To bridge the sim-to-real gap, we introduce a real-to-sim audio hallucination technique, injecting real-world audio samples into simulated scenes with ground-truth contact labels. The resulting multimodal model accurately estimates both the location and size of extrinsic contacts across a range of cluttered and occluded scenarios. We further demonstrate that explicit contact prediction significantly improves policy learning for downstream contact-rich manipulation tasks.
                    </p>
                </div>
            </div><br>

            <!-- Method Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Method</h3>
                    <p style="text-align: justify;">
                        Our method addresses the challenges of extrinsic contact estimation by combining the strengths of multiple sensing modalities. The system consists of three main components that work together to accurately detect and localize contacts between a grasped object and the environment.
                    </p>
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="img/method.jpg" alt="Method Overview" class="img-fluid" style="width: 80%; margin: 20px auto;">
                        </div>
                    </div>
                    <p style="text-align: justify;">
                        <strong>1. Active Audio Sensing:</strong> We equip the robot gripper with contact microphones and conduction speakers. One finger acts as an actuator (speaker) emitting controlled acoustic signals (sweeping impulse signals), while the other finger serves as a receptor (microphone) receiving the acoustic feedback. This setup enables contact perception even in static scenarios, as contact with the environment alters the received signal due to energy absorption.
                    </p>
                    <p style="text-align: justify;">
                        <strong>2. Real-to-Sim Audio Hallucination:</strong> To bridge the sim-to-real gap, we collect audio signals from real-world robot manipulations with simple contact type labels (free, point, line, patch). We then inject these real-world audio samples into simulated scenes with ground-truth contact labels. This technique allows us to train our model entirely in simulation while maintaining robust performance in real-world scenarios.
                    </p>
                    <p style="text-align: justify;">
                        <strong>3. Multimodal Fusion Network:</strong> Our model architecture consists of three UNet encoder streams processing different data: (1) a reference depth image of the object when lifted without contact, (2) current depth and optical flow images cropped around the projected end-effector coordinates, and (3) log-mel spectrograms from audio waveforms. These features are fused with robot pose data to predict per-pixel contact probability maps that indicate both location and type of contact.
                    </p>
                </div>
            </div><br>

            <!-- Features Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Key Features</h3>
                    <div class="row">
                        <div class="col-md-4">
                            <div class="feature-box">
                                <h4>Active Audio Sensing</h4>
                                <p>Uses conduction speakers and contact microphones to detect contacts even in static scenarios and when visual occlusions are present, providing rich information about contact dynamics that is often invisible to visual sensors.</p>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="feature-box">
                                <h4>Occlusion Handling</h4>
                                <p>Maintains high performance even when contact points are visually occluded or in ambiguous near-contact scenarios, thanks to the complementary information provided by audio signals that can travel through solid objects.</p>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="feature-box">
                                <h4>Sim-to-Real Transfer</h4>
                                <p>Our real-to-sim audio hallucination technique enables zero-shot transfer from simulation to real-world scenarios, eliminating the need for real-world training data with contact labels while maintaining robust performance.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div><br>

            <!-- Results Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Results</h3>
                    <p style="text-align: justify;">
                        We evaluated VA2CONTACT on a variety of contact estimation tasks in both simulated and real-world environments, comparing it against the vision-only Im2Contact method and ablation studies. Our experiments demonstrate that our multimodal approach significantly outperforms vision-only methods, especially in challenging scenarios.
                    </p>
                    <div class="row">
                        <div class="col-md-12 text-center">
                            <img src="img/results.jpg" alt="Experimental Results" class="img-fluid" style="width: 80%; margin: 20px auto;">
                        </div>
                    </div>
                    <p style="text-align: justify;">
                        Key findings include:
                    </p>
                    <ul style="text-align: left;">
                        <li>Higher recall and F1 scores in general contact detection, indicating superior ability to detect true contacts with fewer false negatives</li>
                        <li>Accurate contact detection even in heavily occluded scenarios where vision-only methods fail</li>
                        <li>Better performance in ambiguous near-contact scenarios where visual information is insufficient</li>
                        <li>Successful zero-shot transfer from simulation to real-world without additional training</li>
                        <li>Improved performance in downstream manipulation tasks: a contact-aware diffusion policy for a wiping task achieved 8/10 success rate compared to 4/10 for a vision-only baseline</li>
                    </ul>
                </div>
            </div><br>

            <!-- Team Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Team</h3>
                    <div class="row">
                        <div class="col-md-4 text-center">
                            <img src="img/place_holder.jpg" alt="Xili Yi" class="img-fluid rounded-circle" style="width: 150px; height: 150px; object-fit: cover; margin-bottom: 15px;">
                            <h4>Xili Yi</h4>
                            <p>University of Michigan</p>
                            <p><a href="mailto:yixili@umich.edu">yixili@umich.edu</a></p>
                        </div>
                        <div class="col-md-4 text-center">
                            <img src="img/place_holder.jpg" alt="Jayjun Lee" class="img-fluid rounded-circle" style="width: 150px; height: 150px; object-fit: cover; margin-bottom: 15px;">
                            <h4>Jayjun Lee</h4>
                            <p>University of Michigan</p>
                            <p><a href="mailto:jayjun@umich.edu">jayjun@umich.edu</a></p>
                        </div>
                        <div class="col-md-4 text-center">
                            <img src="img/place_holder.jpg" alt="Nima Fazeli" class="img-fluid rounded-circle" style="width: 150px; height: 150px; object-fit: cover; margin-bottom: 15px;">
                            <h4>Nima Fazeli</h4>
                            <p>University of Michigan</p>
                            <p><a href="mailto:nfz@umich.edu">nfz@umich.edu</a></p>
                        </div>
                    </div>
                </div>
            </div><br>

            <!-- Citation Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Citation</h3>
                    <div class="highlight-section">
                        <pre style="text-align: left; background: none; border: none; padding: 10px;">
@inproceedings{yi2025va2contact,
  title={VA2CONTACT: Visual-auditory Extrinsic Contact Estimation},
  author={Yi, Xili and Lee, Jayjun and Fazeli, Nima},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2025}
}
                        </pre>
                    </div>
                </div>
            </div><br>

            <!-- Contact Section -->
            <div class="row">
                <div class="col-md-8 offset-md-2">
                    <h3>Contact</h3>
                    <p>
                        For questions about the project, please contact <a href="mailto:yixili@umich.edu">Xili Yi</a> or <a href="mailto:nfz@umich.edu">Nima Fazeli</a>.
                    </p>
                </div>
            </div>
        </main>

        <footer class="mt-5 pt-5 text-center text-muted">
            <p>© 2025 VA2CONTACT Project | University of Michigan</p>
        </footer>
    </div>
</body>

</html>
